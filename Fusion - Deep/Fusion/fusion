import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import json
import logging
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import torchvision.transforms as transforms
from sklearn.metrics import classification_report, f1_score, confusion_matrix
from transformers import BertTokenizer, BertModel
from torchvision.models import resnet34
from tqdm import tqdm
import time

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("fusion_model.log"),
        logging.StreamHandler()
    ]
)

# Get the directory where the script is located
try:
    script_dir = os.path.dirname(os.path.abspath(__file__))
except NameError:
    # If running in a notebook
    script_dir = os.getcwd()

# Set paths
base_dir = script_dir
data_dir = os.path.join(base_dir, "processed_data")
models_dir = os.path.join(base_dir, "models")
results_dir = os.path.join(base_dir, "results")

# Create directories
os.makedirs(os.path.join(models_dir, "fusion"), exist_ok=True)
os.makedirs(os.path.join(results_dir, "fusion"), exist_ok=True)

#============ DATASET CLASS ============#

class FusionDataset(Dataset):
    """Dataset for fusion model with both text and image data"""
    
    def __init__(self, df, tokenizer, transform=None, max_length=128, label_to_idx=None):
        self.df = df
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # Set up image transforms
        if transform is not None:
            self.transform = transform
        else:
            self.transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        
        # Get label mapping
        if label_to_idx is None:
            # Extract all unique genres
            all_genres = set()
            for genres_list in df['groups']:
                if isinstance(genres_list, list):
                    all_genres.update(genres_list)
            
            all_genres = sorted(list(all_genres))
            self.label_to_idx = {genre: i for i, genre in enumerate(all_genres)}
        else:
            self.label_to_idx = label_to_idx
        
        self.num_labels = len(self.label_to_idx)
        logging.info(f"Dataset initialized with {self.num_labels} possible labels")
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        
        # Process text
        text = row['overview'] if 'overview' in row and pd.notna(row['overview']) else ""
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # Process image
        image_tensor = torch.zeros((3, 224, 224))  # Default empty tensor
        try:
            if 'poster_path' in row and pd.notna(row['poster_path']):
                img_path = row['poster_path']
                if os.path.exists(img_path):
                    img = Image.open(img_path).convert('RGB')
                    image_tensor = self.transform(img)
        except Exception as e:
            logging.warning(f"Error loading image for idx {idx}: {e}")
        
        # Create label vector
        label_vector = torch.zeros(self.num_labels)
        try:
            if isinstance(row['groups'], list):
                for genre in row['groups']:
                    if genre in self.label_to_idx:
                        label_vector[self.label_to_idx[genre]] = 1
        except KeyError:
            logging.warning(f"Groups not found for sample idx {idx}. Using empty label vector.")
        
        # Extract additional features
        metadata = []
        for feature in ['vote_average', 'title_sentiment', 'overview_sentiment']:
            if feature in row and pd.notna(row[feature]):
                metadata.append(float(row[feature]))
            else:
                metadata.append(0.0)
        
        metadata_tensor = torch.tensor(metadata, dtype=torch.float)
        
        return {
            'input_ids': encoding['input_ids'][0],
            'attention_mask': encoding['attention_mask'][0],
            'image': image_tensor,
            'metadata': metadata_tensor,
            'labels': label_vector
        }

#============ MODEL ARCHITECTURE ============#

class TextEncoder(nn.Module):
    """BERT-based text encoder for movie overviews"""
    
    def __init__(self, freeze_bert=True):
        super().__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        
        # Freeze BERT layers to prevent overfitting
        if freeze_bert:
            for param in self.bert.parameters():
                param.requires_grad = False
            
            # Unfreeze only the last 2 layers for fine-tuning
            for param in self.bert.encoder.layer[-2:].parameters():
                param.requires_grad = True
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        return outputs.pooler_output  # [batch_size, 768]

class ImageEncoder(nn.Module):
    """ResNet-based image encoder for movie posters"""
    
    def __init__(self, freeze_backbone=True):
        super().__init__()
        # Load pretrained ResNet34
        self.backbone = resnet34(pretrained=True)
        
        # Remove classification layer
        self.features = nn.Sequential(*list(self.backbone.children())[:-1])
        
        # Freeze backbone
        if freeze_backbone:
            for param in self.features.parameters():
                param.requires_grad = False
            
            # Unfreeze last few layers
            layers_to_unfreeze = list(self.features.children())[-3:]
            for layer in layers_to_unfreeze:
                for param in layer.parameters():
                    param.requires_grad = True
    
    def forward(self, x):
        features = self.features(x)
        features = features.view(features.size(0), -1)  # Flatten: [batch_size, 512]
        return features

class FusionModel(nn.Module):
    """Model that fuses text, image, and metadata for genre classification"""
    
    def __init__(self, num_genres, dropout=0.3):
        super().__init__()
        
        # Text and image encoders
        self.text_encoder = TextEncoder(freeze_bert=True)
        self.image_encoder = ImageEncoder(freeze_backbone=True)
        
        # Dimensions
        text_dim = 768  # BERT output
        image_dim = 512  # ResNet34 output
        metadata_dim = 3  # vote_average, title_sentiment, overview_sentiment
        
        # Metadata processor
        self.metadata_processor = nn.Sequential(
            nn.Linear(metadata_dim, 64),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # Feature fusion - combine all modalities
        fusion_input_dim = text_dim + image_dim + 64  # Text + Image + Metadata
        
        # Attention mechanism for feature importance
        self.attention = nn.Sequential(
            nn.Linear(fusion_input_dim, fusion_input_dim // 4),
            nn.ReLU(),
            nn.Linear(fusion_input_dim // 4, fusion_input_dim),
            nn.Sigmoid()
        )
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.Linear(fusion_input_dim, 512),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.BatchNorm1d(512),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.BatchNorm1d(256),
            nn.Linear(256, num_genres)
        )
    
    def forward(self, input_ids, attention_mask, image, metadata):
        # Extract features from each modality
        text_features = self.text_encoder(input_ids, attention_mask)
        image_features = self.image_encoder(image)
        metadata_features = self.metadata_processor(metadata)
        
        # Concatenate features
        combined = torch.cat([text_features, image_features, metadata_features], dim=1)
        
        # Apply attention to weight features based on importance
        attention_weights = self.attention(combined)
        attended_features = combined * attention_weights
        
        # Classification
        logits = self.classifier(attended_features)
        
        return logits

class FocalLoss(nn.Module):
    """Focal loss for handling class imbalance"""
    
    def __init__(self, gamma=2.0, alpha=0.25):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha
    
    def forward(self, logits, targets):
        BCE_loss = nn.BCEWithLogitsLoss(reduction='none')(logits, targets)
        pt = torch.exp(-BCE_loss)  # prevents nans when probability 0
        
        # Apply weights to each class
        alpha_weight = self.alpha * targets + (1 - self.alpha) * (1 - targets)
        
        # Apply gamma focusing parameter
        focal_loss = alpha_weight * (1 - pt) ** self.gamma * BCE_loss
        
        return focal_loss.mean()

#============ TRAINING AND EVALUATION FUNCTIONS ============#

def train_epoch(model, dataloader, optimizer, criterion, device):
    """Train model for one epoch"""
    model.train()
    epoch_loss = 0
    
    progress_bar = tqdm(dataloader, desc="Training")
    for batch in progress_bar:
        # Get data
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        images = batch['image'].to(device)
        metadata = batch['metadata'].to(device)
        labels = batch['labels'].to(device)
        
        # Zero gradients
        optimizer.zero_grad()
        
        # Forward pass
        logits = model(input_ids, attention_mask, images, metadata)
        
        # Calculate loss
        loss = criterion(logits, labels)
        
        # Backward pass
        loss.backward()
        
        # Clip gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # Update parameters
        optimizer.step()
        
        # Update metrics
        epoch_loss += loss.item()
        
        # Update progress bar
        progress_bar.set_postfix({'loss': f"{loss.item():.4f}"})
    
    return epoch_loss / len(dataloader)

def evaluate(model, dataloader, criterion, device, threshold=0.4):
    """Evaluate model on dataloader"""
    model.eval()
    epoch_loss = 0
    
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            # Get data
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            images = batch['image'].to(device)
            metadata = batch['metadata'].to(device)
            labels = batch['labels'].to(device)
            
            # Forward pass
            logits = model(input_ids, attention_mask, images, metadata)
            
            # Calculate loss
            loss = criterion(logits, labels)
            
            # Convert logits to predictions
            predictions = (torch.sigmoid(logits) > threshold).float()
            
            # Update metrics
            epoch_loss += loss.item()
            
            # Store predictions and labels
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # Calculate F1 score
    f1_micro = f1_score(all_labels, all_predictions, average='micro')
    f1_macro = f1_score(all_labels, all_predictions, average='macro')
    
    return epoch_loss / len(dataloader), f1_micro, f1_macro, all_predictions, all_labels

def optimize_threshold(model, dataloader, device, thresholds=None):
    """Find optimal threshold for each class"""
    model.eval()
    
    all_labels = []
    all_probs = []
    
    # If no thresholds provided, start with default 0.5
    if thresholds is None:
        num_classes = next(iter(dataloader))['labels'].shape[1]
        thresholds = [0.5] * num_classes
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Finding optimal thresholds"):
            # Get data
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            images = batch['image'].to(device)
            metadata = batch['metadata'].to(device)
            labels = batch['labels'].to(device)
            
            # Forward pass
            logits = model(input_ids, attention_mask, images, metadata)
            probs = torch.sigmoid(logits)
            
            # Store
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
    
    # Convert to numpy arrays
    labels_array = np.array(all_labels)
    probs_array = np.array(all_probs)
    
    # Find optimal threshold for each class
    optimal_thresholds = []
    
    for i in range(labels_array.shape[1]):
        best_f1 = 0
        best_threshold = 0.5
        
        # Try different thresholds
        for threshold in np.arange(0.2, 0.8, 0.05):
            preds = (probs_array[:, i] > threshold).astype(float)
            f1 = f1_score(labels_array[:, i], preds, zero_division=0)
            
            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold
        
        optimal_thresholds.append(best_threshold)
    
    return optimal_thresholds

def train_fusion_model():
    """Train and evaluate the fusion model"""
    
    # Set random seeds for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)
    
    # Settings
    BATCH_SIZE = 16
    LEARNING_RATE = 2e-5
    WEIGHT_DECAY = 0.01
    DROPOUT = 0.3
    N_EPOCHS = 15
    MAX_TEXT_LENGTH = 128
    PATIENCE = 5  # Early stopping patience
    
    # Select device
    if torch.backends.mps.is_available():
        device = torch.device("mps")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    logging.info(f"Using device: {device}")
    
    # Load data
    logging.info("Loading data...")
    try:
        train_df = pd.read_pickle(os.path.join(data_dir, "splits", "train.pkl"))
        val_df = pd.read_pickle(os.path.join(data_dir, "splits", "val.pkl"))
        test_df = pd.read_pickle(os.path.join(data_dir, "splits", "test.pkl"))
        logging.info(f"Loaded {len(train_df)} training, {len(val_df)} validation, and {len(test_df)} test samples")
        
        # Filter rows without posters
        train_df = train_df[train_df['poster_path'].notna()]
        val_df = val_df[val_df['poster_path'].notna()]
        test_df = test_df[test_df['poster_path'].notna()]
        
        logging.info(f"After filtering for posters: {len(train_df)} training, {len(val_df)} validation, and {len(test_df)} test samples")
        
        # Load or create genre mapping
        try:
            with open(os.path.join(data_dir, "genre_mapping.json"), 'r') as f:
                genre_mapping = json.load(f)
                label_to_idx = genre_mapping
                idx_to_label = {int(idx): label for label, idx in genre_mapping.items()}
        except:
            # Define the top genres we want to classify
            TOP_GENRES = [
                'Drama', 'Comedy', 'Thriller', 'Action', 'Romance',
                'Horror', 'Adventure', 'Crime', 'Science Fiction', 'Family'
            ]
            
            label_to_idx = {genre: i for i, genre in enumerate(TOP_GENRES)}
            idx_to_label = {i: genre for i, genre in enumerate(TOP_GENRES)}
            
            # Save mapping
            with open(os.path.join(models_dir, "fusion", "genre_mapping.json"), 'w') as f:
                json.dump(label_to_idx, f, indent=2)
        
        logging.info(f"Using {len(label_to_idx)} genres: {list(label_to_idx.keys())}")
        
    except Exception as e:
        logging.error(f"Error loading data: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return
    
    # Initialize BERT tokenizer
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    
    # Create data augmentation for training
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomCrop((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # Create datasets
    train_dataset = FusionDataset(
        train_df, tokenizer, transform=train_transform, 
        max_length=MAX_TEXT_LENGTH, label_to_idx=label_to_idx
    )
    
    val_dataset = FusionDataset(
        val_df, tokenizer, transform=val_transform, 
        max_length=MAX_TEXT_LENGTH, label_to_idx=label_to_idx
    )
    
    test_dataset = FusionDataset(
        test_df, tokenizer, transform=val_transform, 
        max_length=MAX_TEXT_LENGTH, label_to_idx=label_to_idx
    )
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True, 
        num_workers=2 if torch.cuda.is_available() else 0, pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset, batch_size=BATCH_SIZE, 
        num_workers=2 if torch.cuda.is_available() else 0, pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset, batch_size=BATCH_SIZE, 
        num_workers=2 if torch.cuda.is_available() else 0, pin_memory=True
    )
    
    # Create model
    logging.info("Creating fusion model...")
    model = FusionModel(
        num_genres=len(label_to_idx),
        dropout=DROPOUT
    )
    
    # Move model to device
    model = model.to(device)
    
    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logging.info(f"Model has {trainable_params:,} trainable parameters out of {total_params:,} total parameters")
    logging.info(f"Percentage trainable: {trainable_params/total_params*100:.2f}%")
    
    # Create loss function
    criterion = FocalLoss(gamma=2.0, alpha=0.25)
    
    # Create optimizer with different learning rates for different parts
    # Higher LR for classifiers, lower for encoders
    param_groups = [
        # Text encoder - lowest LR
        {'params': [p for n, p in model.text_encoder.named_parameters() if p.requires_grad], 
         'lr': LEARNING_RATE / 10},
        # Image encoder - lowest LR
        {'params': [p for n, p in model.image_encoder.named_parameters() if p.requires_grad], 
         'lr': LEARNING_RATE / 10},
        # Metadata processor - mid LR
        {'params': model.metadata_processor.parameters(), 
         'lr': LEARNING_RATE / 2},
        # Attention and classifier - highest LR
        {'params': list(model.attention.parameters()) + list(model.classifier.parameters()), 
         'lr': LEARNING_RATE}
    ]
    
    optimizer = optim.AdamW(param_groups, weight_decay=WEIGHT_DECAY)
    
    # Learning rate scheduler
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=2, verbose=True
    )
    
    # Training loop
    logging.info("Starting training...")
    start_time = time.time()
    
    best_val_f1 = 0.0
    best_model_state = None
    patience_counter = 0
    
    # Track metrics
    train_losses = []
    val_losses = []
    val_f1s_micro = []
    val_f1s_macro = []
    
    for epoch in range(N_EPOCHS):
        epoch_start = time.time()
        
        # Train
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
        train_losses.append(train_loss)
        
        # Evaluate
        val_loss, val_f1_micro, val_f1_macro, val_preds, val_labels = evaluate(
            model, val_loader, criterion, device
        )
        val_losses.append(val_loss)
        val_f1s_micro.append(val_f1_micro)
        val_f1s_macro.append(val_f1_macro)
        
        epoch_time = time.time() - epoch_start
        
        # Log metrics
        logging.info(f"Epoch {epoch+1}/{N_EPOCHS} - Time: {epoch_time:.2f}s")
        logging.info(f"  Train Loss: {train_loss:.4f}")
        logging.info(f"  Val Loss: {val_loss:.4f}, Val F1 (micro): {val_f1_micro:.4f}, Val F1 (macro): {val_f1_macro:.4f}")
        
        # Update learning rate based on validation F1
        scheduler.step(val_f1_micro)
        
        # Save best model
        if val_f1_micro > best_val_f1:
            logging.info(f"  New best model! F1: {val_f1_micro:.4f} (previous: {best_val_f1:.4f})")
            best_val_f1 = val_f1_micro
            best_model_state = model.state_dict().copy()
            patience_counter = 0
        else:
            patience_counter += 1
            logging.info(f"  No improvement for {patience_counter} epochs")
            
            if patience_counter >= PATIENCE:
                logging.info(f"Early stopping after {epoch+1} epochs")
                break
        
        # Save checkpoint every 5 epochs
        if (epoch + 1) % 5 == 0:
            torch.save(model.state_dict(), os.path.join(models_dir, "fusion", f"checkpoint_epoch_{epoch+1}.pt"))
    
    total_time = time.time() - start_time
    logging.info(f"Training completed in {total_time:.2f}s")
    
    # Load best model
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
    
    # Save best model
    torch.save(model.state_dict(), os.path.join(models_dir, "fusion", "best_model.pt"))
    
    # Save genre mapping
    with open(os.path.join(models_dir, "fusion", "genre_mapping.json"), 'w') as f:
        json.dump(label_to_idx, f, indent=2)
    
    # Plot training curves
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.plot(train_losses, label='Train')
    plt.plot(val_losses, label='Validation')
    plt.title('Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    plt.subplot(1, 3, 2)
    plt.plot(val_f1s_micro, label='Micro F1')
    plt.plot(val_f1s_macro, label='Macro F1')
    plt.title('Validation F1 Score')
    plt.xlabel('Epoch')
    plt.ylabel('F1 Score')
    plt.legend()
    
    plt.subplot(1, 3, 3)
    for i, group in enumerate(optimizer.param_groups):
        plt.text(0.5, 0.8-i*0.2, f"Group {i} LR: {group['lr']:.2e}", ha='center', va='center', fontsize=10)
    plt.title('Learning Rates')
    plt.axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, "fusion", "training_curves.png"))
    
    # Find optimal thresholds
    logging.info("Finding optimal thresholds for each class...")
    optimal_thresholds = optimize_threshold(model, val_loader, device)
    
    threshold_dict = {idx_to_label[i]: float(thresh) for i, thresh in enumerate(optimal_thresholds)}
    logging.info(f"Optimal thresholds: {threshold_dict}")
    
    # Save thresholds
    with open(os.path.join(models_dir, "fusion", "optimal_thresholds.json"), 'w') as f:
        json.dump(threshold_dict, f, indent=2)
    
    # Evaluate on test set with optimal thresholds
    logging.info("Evaluating on test set with optimal thresholds...")
    test_loss, test_f1_micro, test_f1_macro, test_preds, test_labels = evaluate(
        model, test_loader, criterion, device, threshold=0.4  # Default threshold - will use class-specific below
    )
    
    # Apply optimal thresholds to raw probabilities
    test_probs = []
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Getting probabilities"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            images = batch['image'].to(device)
            metadata = batch['metadata'].to(device)
            
            logits = model(input_ids, attention_mask, images, metadata)
            probs = torch.sigmoid(logits)
            test_probs.extend(probs.cpu().numpy())
    
    # Apply class-specific thresholds
    test_probs_array = np.array(test_probs)
    test_labels_array = np.array(test_labels)
    
    # Apply optimal thresholds
    thresholded_preds = np.zeros_like(test_probs_array)
    for i, threshold in enumerate(optimal_thresholds):
        thresholded_preds[:, i] = (test_probs_array[:, i] > threshold).astype(float)
    
    # Calculate metrics with optimal thresholds
    test_f1_micro_opt = f1_score(test_labels_array, thresholded_preds, average='micro')
    test_f1_macro_opt = f1_score(test_labels_array, thresholded_preds, average='macro')
    
    logging.info(f"Test Results:")
    logging.info(f"  Loss: {test_loss:.4f}")
    logging.info(f"  F1 (micro) with default threshold: {test_f1_micro:.4f}")
    logging.info(f"  F1 (micro) with optimal thresholds: {test_f1_micro_opt:.4f}")
    logging.info(f"  F1 (macro) with optimal thresholds: {test_f1_macro_opt:.4f}")
    
    # Calculate per-class F1 scores
    per_class_f1 = f1_score(test_labels_array, thresholded_preds, average=None)
    
    # Log per-class F1 scores
    logging.info("Per-class F1 scores:")
    class_metrics = {}
    for i, f1 in enumerate(per_class_f1):
        genre = idx_to_label[i]
        support = np.sum(test_labels_array[:, i])
        threshold = optimal_thresholds[i]
        logging.info(f"  {genre}: F1={f1:.4f}, Support={support}, Threshold={threshold:.4f}")
        class_metrics[genre] = {"f1": float(f1), "support": int(support), "threshold": float(threshold)}
    
    # Save results
    results = {
        "test_f1_micro": float(test_f1_micro_opt),
        "test_f1_macro": float(test_f1_macro_opt),
        "train_time": float(total_time),
        "best_val_f1": float(best_val_f1),
        "per_class_metrics": class_metrics
    }
    
    with open(os.path.join(results_dir, "fusion", "test_results.json"), 'w') as f:
        json.dump(results, f, indent=2)
    
    # Plot per-class F1 scores
    plt.figure(figsize=(10, 6))
    genres = []
    f1_scores = []
    
    for genre, metrics in sorted(class_metrics.items(), key=lambda x: x[1]['f1'], reverse=True):
        genres.append(genre)
        f1_scores.append(metrics['f1'])
    
    plt.barh(genres, f1_scores)
    plt.xlabel('F1 Score')
    plt.title('F1 Score by Genre')
    plt.xlim(0, 1.0)
    
    # Add values on bars
    for i, v in enumerate(f1_scores):
        plt.text(v + 0.01, i, f"{v:.4f}", va='center')
    
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, "fusion", "per_class_f1.png"))
    
    # Create confusion matrices for each genre
    plt.figure(figsize=(15, 10))
    
    # Plot 3x4 grid of confusion matrices
    rows, cols = 3, 4  # Adjust as needed
    
    for i, genre in enumerate(label_to_idx.keys()):
        if i >= rows * cols:
            break
            
        class_idx = label_to_idx[genre]
        y_true = test_labels_array[:, class_idx]
        y_pred = thresholded_preds[:, class_idx]
        
        # Create confusion matrix
        cm = confusion_matrix(y_true, y_pred)
        
        plt.subplot(rows, cols, i + 1)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
                   xticklabels=['Negative', 'Positive'],
                   yticklabels=['Negative', 'Positive'])
        
        plt.title(f'{genre}')
        plt.xlabel('Predicted')
        plt.ylabel('True')
    
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, "fusion", "confusion_matrices.png"))
    
    return model, results

if __name__ == "__main__":
    train_fusion_model()